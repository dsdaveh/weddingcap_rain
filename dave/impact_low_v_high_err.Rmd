---
title: "Strategy Question - Improve prediction accuracy or simulate spurious errors?"
author: "Dave Hurst"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: 
    html_document:
        css: standard.css

---

## Should we be trying to simulate the large error or improve the plausible predictions?

```{r libs, echo=FALSE, message=FALSE, warning=FALSE}

library(dplyr)
library(data.table)
library(Metrics)
library(ggvis)
library(xtable)

source("../team/rain_utils.R")


rdata_file <- "train_full.RData" # "train_10pct.RData" # 
if (!exists("train")) {
    tcheck(0)
    if (file.exists( rdata_file )) {
        cat("loading train from RData file ", rdata_file, "\n")
        load( rdata_file )
    } else {
        cat("loading train from CSV\n")
        train <- fread("../train.csv")
    }
}
#tcheck()

cuts <- c( 10, 30, 70 )

```

First let's look at the cummulative distribution of the error sorted by the measured rain value.  We can see that the extreme high error values account for well over 90% of the overall error and correspond to higher measured values.  The measured value thresholds are marked by the blue lines at y = Expected = `r cuts`;

```{r pop, echo=FALSE, message=FALSE}
rainfall <- train[ ,.(
    yhat = mpalmer(Ref, minutes_past)
    , y = max(Expected)
), Id][ yhat >= 0, ]

rainfall$abs_err <- abs( rainfall$yhat - rainfall$y )

# setkey( rainfall, abs_err)
# rainfall$err_cdf_pct <- 100 * cumsum( rainfall$abs_err) / sum( rainfall$abs_err)
# 
# # plot( rainfall$err_cdf, type = "l")
# rainfall %>% mutate( index=row_number( err_cdf_pct ) ) %>% ggvis( ~index, ~err_cdf_pct ) %>%
#     layer_lines() 

setkey( rainfall, y)
cut1 <- which(rainfall$y >cuts[1])[1]
cut2 <- which(rainfall$y >cuts[2])[1]
cut3 <- which(rainfall$y >cuts[3])[1]

rainfall$yerr_cdf_pct <- 100 * cumsum( rainfall$abs_err) / sum( rainfall$abs_err)

plot( sort( rainfall$yerr_cdf_pct), type="l", ylab = "Cumulative Error %")
abline( v = cut1, lty = 2, col = "blue")
abline( v = cut2, lty = 3, col = "blue")
abline( v = cut3, lty = 4, col = "blue")
legend( "topleft", lty = c(2,3,4), legend = c("10 mm", "30 mm", "70mm"))
# rainfall %>% mutate( index=row_number( y ) ) %>% ggvis( ~index, ~yerr_cdf_pct ) %>%
#     layer_lines() %>%
#     layer_lines( x=cut1, stroke := "blue", strokeDash := 10) %>%
#     layer_lines( x=cut2, stroke := "blue", strokeDash := 5 ) %>%
#     layer_lines( x=cut3, stroke := "blue", strokeDash := 1 ) 

mae_trn <- with(rainfall, mae( yhat, y)  )  #23.4304

#tcheck()
```

The total MAE for the dataset is `r mae_trn`.  

Let's take a look at the MAE for the dataset at the different thresholds of the measured rain value.  The table below gives the MAE below the threshold value (i.e. to the left of the threshold line in the plot above), above the value (to the right of the line).  The 'fix'  values below the threshold set error = 0 and calculate the threshold for the entire dateset.  Conversely, the 'fix' values above the dataset change the higher values by averaging between the predicted and measured values and calculate the MAE for the dataset with original predictions below the theshold.

```{r cuts, echo=FALSE, results='asis'}

cheat_full <- with(rainfall, mae( ( yhat + y) /2, y) )

df <- data.frame()
    df <- rbind( data.frame(
        Threshold = 0
        , Fraction_Below = 0.0
        , MAE_Below = 0
        , MAE_Above = mae_trn
        , MAE_After_Fix_Below = NA
        , MAE_After_Fix_Above = cheat_full
    ))

for (i in 1:length(cuts)) {
    cut_left <- rainfall[ y < cuts[i], ]
    mae_left <- with(cut_left, mae( yhat, y)  )  

    cut_right <- rainfall[ y >= cuts[i], ][ yhat >= 0, ]
    mae_right <- with(cut_right, mae( yhat, y)  )  
    
    #fix left hand values 
    cheat <- cut_left
    cheat$yhat <- cheat$y
    cheat <- rbind( cheat, cut_right)
    cheat_left <- with(cheat, mae( yhat, y))
    
    #improve right hand values 
    cheat <- cut_right
    cheat$yhat <- ( cheat$y + cheat$yhat ) / 2
    cheat <- rbind( cut_left, cheat)
    cheat_right<- with(cheat, mae( yhat, y))
    
    df <- rbind( df, data.frame(
        Threshold = cuts[i]
        , Fraction_Below = nrow(cut_left) / nrow(rainfall)
        , MAE_Below = mae_left
        , MAE_Above = mae_right
        , MAE_After_Fix_Below = cheat_left
        , MAE_After_Fix_Above = cheat_right
    ))
}
       print( xtable( df ), type='html')



```

## Conclusion

Improving the meteorological predictions for the real data will have very low impact on improving the score on the contest leaderboard, since the MAE is introduced by the unrealistically high measured values.  We might still want to pursue this avenue since there would be business value, and it appears to be what the contest set out to accomplish.

On the other hand, a small improvement on predicting the large error would yield a much higher leaderboard score, so if we want to do well in the contest we need to try and predict the stations that are more likely to yield unrealistically high measurement values.

--------

File = dave/impact_low_v_high_err.Rmd


